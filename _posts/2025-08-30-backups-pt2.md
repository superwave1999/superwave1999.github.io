---
title: "ZFS + Restic Backup Restoration - Part 2"
date: 2025-08-30
tags:
  - ZFS
  - Restic
  - Backups
  - Linux
  - Disaster Recovery
categories: [documentation]
media_subpath: /assets/posts/2025-08-30-backups-pt2
---

# Prologue

Now, given that we have our backups it's time to make sure everything is running correctly and simulate disaster scenarios. This guide assumes you've read part 1.

## OS drive - Bad update, hacks, ransomware

We will simulate the cases stated above.

First, we will create a file in a system directory, such as:

```bash
sudo touch /opt/hello.txt
```

Create a snapshot that we'll restore to (you can also use my backup script in my repo):

```bash
sudo zfs snapshot -r zroot@restore-proc
sudo zfs destroy zroot/swap@restore-proc
```

"Accidentally" delete our file.

```bash
sudo rm /opt/hello.txt
```

### Restoration process

In this case you'll need to dig up a monitor + keyboard, reboot the server, and press "Tab" on the main option in rEFInd:

![rEFInd main](/refind-main.jpg)

In this "Advanced options" menu, highlight and enter the first "Boot to menu".

![rEFInd advanced](/refind-advanced.jpg)

ZFSBootMenu will load up.

![ZFSBootmenu](/zbm-main.jpg)

First, I'll explain some basic controls:

- Left/Right arrow - Navigate between the Boot, Snapshots, Kernels, Pool Status tabs
- Up/Down arror - Navigate between the available options of the tabs
- ESCAPE - Back a step.
- Or look at the controls on the screen, each explained below using Git terms.

| Action / Key                 | Creates Independent Boot Environment (BE)?          | Uses Snapshot Blocks?         | Changes Persist?    | Safe to Test?            | Permanent? | Typical Use Case                                                        |
| ---------------------------- | -------------------------------- | ----------------------------- | ------------------- | ------------------------ | ---------- | ----------------------------------------------------------------------- |
| **Duplicate (ENTER)**        | ✅ Yes                            | ❌ No                          | ✅ Yes               | ✅ Yes                    | ✅ Yes      | Fork snapshot into a new BE for experimentation, preserves everything   |
| **Clone only (CTRL+C)**      | ❌ No                             | ✅ Yes                         | ❌ Unless promoted   | ✅ Yes                    | ❌ No       | Lightweight, temporary boot into a snapshot for testing                 |
| **Clone + Promote (CTRL+X)** | ✅ Yes                            | ✅ Yes initially → independent | ✅ Yes               | ✅ Yes                    | ✅ Yes      | Safest rollback while keeping history, becomes a new BE                 |
| **New Snapshot (CTRL+N)**    | ❌ — adds snapshot to existing BE | ✅ Snapshot is made            | ✅ Yes               | ✅ Yes                    | ✅ Yes      | Quick restore point before risky changes                                |
| **Show Diff (CTRL+D)**       | ❌ — read-only                    | ✅ Reads snapshot data         | ❌ No                | ✅ Yes                    | ❌ No       | Inspect what changed between snapshots or now                           |
| **View Logs (CTRL+L)**       | ❌ — read-only                    | ❌ N/A                         | ❌ No                | ✅ Yes                    | ❌ No       | Debugging errors in snapshot/boot operations                            |
| **Rollback (CTRL+R)**        | ❌ — restores snapshot            | ✅ Snapshot blocks             | ✅ Yes (destructive) | ❌ Not reversible         | ✅ Yes      | Emergency recovery: wipe newer changes, go back to last safe snapshot   |
| **Jump into chroot**         | ❌ — repairs existing BE          | ❌ N/A (uses live dataset)     | ✅ Yes (in that BE)  | ✅ Yes (safe for repairs) | ✅ Yes      | Fix configs, reinstall packages, reset passwords without booting the BE |

So, for our use case, it's enough to use Rollback (CTRL+R) or Clone & Promote (CTRL+X), depending if we want to keep the newer bad snapshots, if they exist, for investigation.

For this guide, I'll hit CTRL+R.

![ZFSBootmenu snapshots](/zbm-snapshots.jpg)

Scary warning screen since this is indeed destructive of all newer data.

![ZFSBootmenu warning](/zbm-warn.jpg)

Now we should navigate back to the main "Boot menu". Hit enter. Wait a bit. And run the following:

```bash
ls -la /opt
```

We should see our hello.txt file again.

## Data drive(s) option 1 - Hacks, ransomware, user error, bad Docker container

Alas! ZFS saves us again! The caveat to this is that you do need to have a working boot to be able to do this, since ZFSBootmenu will only cover the OS drive. Thankfully, we already restored our OS to a working state in the previous section of this guide :).

Make sure programs using this drive are stopped, just in case. If you've been following my guides, stopping docker should be more than enough.

```bash
sudo systemctl stop docker.service docker.socket
```

Now, we will do the same process of creating our test file, making a snapshot, and deleting the file to simulate a basic disaster.

```bash
sudo touch /storage/hello.txt
sudo zfs snapshot -r datapool@initial-test
sudo rm /storage/hello.txt
```

### Restoration process

Due to the lack of ZFSBootmenu and due to the fact that I haven't bothered setting up datasets, we dont have as many advanced options available. We restore with the following command (warning! destructive of all newer data than the snapshot):

```bash
sudo zfs rollback datapool@initial-test
```

Check your hello.txt file again, it should exist!

## Data drive(s) option 2 - Controller / ZFS / Multiple drive failure

We have our snapshots for the cases mentioned before caused by software, but what if the hardware fails catastropically? Well, we have our Restic setup keeping our data safe in a random place on earth!

As you may expect, this step assumes that the OS is stable and bootable, but the data drive is toast and /storage is likely empty.

Your first step is to re-create or find the pool, and mount it to the same location. I'll leave this to you.

Once we have a pool restored or recreated and potentially empty, it's time to pull our most recent Restic snapshot.

Note: To recreate this scenario, I actually rm -rf'd the directories under my /storage pool. If you see this guide published, it means everything went ok.

### Restoration process

> I have a handy script to do all this for you! Check my github.

Time to restore. We require rsync to be installed.

We can use restics' built in restore command, but mounting + using rsync gives us greater control over the process.

This process is done as root (same as backup creation) to maintain the ownership and permissions of the backed-up files, so sudo su.

We mount our restic repo (the & lets us run restic mount in the background):

```bash
mkdir -p "/mnt/restic"
restic mount "/mnt/restic" &
```

We can now browse /mnt/restic (it's read-only so safe from any accidents). We want to locate our latest snapshot, usually found in /mnt/restic/snapshots/latest. cd to this directory. If you ls -la we will see /storage containing ./files and ./containerdata from our first backup created in the last guide.

Given our case that /storage is empty and want to restore with the same paths, attributes, and data, removing any files that differ, we do the following:

```bash
rsync -aHAX --delete --dry-run -v /mnt/restic/snapshots/latest/storage/ /storage/
```

> In case of doubt, always run this command with --dry-run too to see what would be changed.

Go for a walk, and everything should be restored. To finish, unmount /mnt/restic.

```bash
umount -u /mnt/restic
```

# Epilogue

These processes are robust enough and shouldn't take much time to restore except the OS drive failure sadly. Not much can be done, but this shouldn't happen in a few years with a good drive.
